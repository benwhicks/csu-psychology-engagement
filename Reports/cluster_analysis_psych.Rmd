---
title: "Cluster Assessment for Psychology Engagement Research"
author: Ben Hicks
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, message = F, warning = F, cache = T, fig.width = 8)
#knitr::opts_chunk$set(dev = 'png') - - uncomment to reduce file size
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(GGally))
suppressPackageStartupMessages(library(ggraph))
suppressPackageStartupMessages(library(ggthemes))
suppressPackageStartupMessages(library(ggpubr))
suppressPackageStartupMessages(library(lakit))
suppressPackageStartupMessages(library(dendextend))
suppressPackageStartupMessages(library(ggsci))
suppressPackageStartupMessages(library(factoextra))
suppressPackageStartupMessages(library(cluster))
suppressPackageStartupMessages(library(rgl))

knitr::knit_hooks$set(webgl = hook_webgl)
```

``` {r importing, cache=TRUE}
source(file.path('..','R','psych_functions.R'))
source(file.path('..', 'R', 'import-data.R'))
```


``` {r lazy_names}
# Making short names for the data sets for convinience
d <- nodeMatrix
dBio <- nodeMatrixBio
dSoc <- nodeMatrixSoc
dAct <- nodeMatrix[, 2:8]
dBioAct <- nodeMatrixBio[, 2:8]
dSocAct <- nodeMatrixSoc[, 2:8]
dGrd <- as.matrix(nodeMatrix[, 1])
dBioGrd <- as.matrix(nodeMatrixBio[, 1])
dSocGrd <- as.matrix(nodeMatrixSoc[, 1])
dlist <- list(d, dBio, dSoc, dAct, dBioAct, dSocAct, dGrd, dBioGrd, dSocGrd)
```

The aim of this document is to explore the options of clustering students in an online course into groups based on their activity data, academic performance, and then both together. There are two subjects analysed (Biopsychology and Social Psychology) and two types of data on each (activity or academic) so we explore 9 possibilities for each clustering method, as we can merge the data or subjects together as an option as well. Of interest is the variance amongst the clustering methods and what is a good choice to provide distinct groups based on the summary activity and academic performance of those groups. 

Example code on R cluster analysis link below[^1] and here[^3], and for PCA here[^4] and here[^5].

# Principal Component Analysis

## PCA all data, both subjects

``` {r pca_1}
pc <- prcomp(d)
knitr::kable(pc$rotation, digits = 3)
summary(pc)
```

## Activity data, both subjects

``` {r pca_2}
pcAct <- prcomp(dAct)
knitr::kable(pcAct$rotation, digits = 3)
summary(pcAct)
```

## PCA Remarks

We can see that the activity data on it's own explains more proportion of variance over the first few components than with the *Grade* variable included, indicating it might prove useful to exclude the *Grade* data when clustering, which will then provide insights on how the type of activity matches with academic performance.

# Ideal number of clusters

We use the gap statistic[^2] to first measure the suggested number of clusters using heirarchical and k-means methods on each of the data sets.

### K-medoids cluster number optimisation - all options

``` {r k_means_k_test, cache = TRUE}
kmp <- function(data, sub, method = "gap_stat") {
  fviz_nbclust(data, cluster::pam, method = method) +
    ggtitle(label = 'Optimal number of clusters, k-medoids', 
            subtitle = sub)   +
     scale_y_continuous(limits = c(0.2, 1.45))
}

ggarrange(kmp(d,    'Both subjects, all data'), 
          kmp(dBio, 'Biopsychology, all data'),
          kmp(dSoc, "Social Psychology, all data"),
          kmp(dAct, 'Both subjects, activity data'),
          kmp(dBioAct, 'Biopsychology, activity data'),
          kmp(dSocAct, "Social Psychology, activity data"),
          kmp(dGrd, "Both subjects, academic data"),
          kmp(dBioGrd, "Biopsychology, academic data"),
          kmp(dSocGrd, "Social Psychology, academic data"),
          nrow = 3, ncol = 3,
          font.label = list(size = 9))
```

Note that the academic clustering is giving much lower results for the gap statistic (nearly half). This is not too suprising as the single variable, *Grade*, although not quite normal is still centrally distributed (see plot below). For the remainder of the report academic clustering methods will be ignored in favour of using quantiles instead.

``` {r qqplot_for_academic_data}
ggarrange(
  ggqqplot(d[,1], title = "Q-Q Plot for Academic Grades"),
  data.frame(Grades = d[,1]) %>% ggplot(aes(x = Grades)) + geom_density() + theme_minimal() + ggtitle("Academic grade distribution")
, nrow = 1, ncol = 2)

shapiro.test(d[ ,1])
```

Also, it is interesting to note that the gap statistic is higher when excluding the academic data. As such it might be prudent to cluster primarily on the activity data and analyse the academic performance as part of that data set. 

Below is a repeat of the graph above with academic data excluded, to better show the variation.

### K-medoids cluster number optimisation

``` {r k_means_k_test2, cache = TRUE}
kmp <- function(data, sub, method = "gap_stat") {
  fviz_nbclust(data, cluster::pam, method = method) +
    ggtitle(label = 'Optimal number of clusters, k-medoids', 
            subtitle = sub)  +
     scale_y_continuous(limits = c(0.9, 1.45))
}

ggarrange(kmp(d,    'Both subjects, all data'), 
          kmp(dBio, 'Biopsychology, all data'),
          kmp(dSoc, "Social Psychology, all data"),
          kmp(dAct, 'Both subjects, activity data'),
          kmp(dBioAct, 'Biopsychology, activity data'),
          kmp(dSocAct, "Social Psychology, activity data"),
          nrow = 2, ncol = 3,
          font.label = list(size = 9))
```

### Hierarchical cluster number optimisation


``` {r heirarchical_k_test, cache = TRUE, fig.width = 8}
hp <- function(data, sub, method = "gap_stat") {
  fviz_nbclust(data, hcut, method = method) +
    ggtitle(label = 'Optimal number of clusters, hierarchical', 
            subtitle = sub)  +
    scale_y_continuous(limits = c(0.9, 1.45))
}

ggarrange(hp(d,    'Both subjects, all data'), 
          hp(dBio, 'Biopsychology, all data'),
          hp(dSoc, "Social Psychology, all data"),
          hp(dAct, 'Both subjects, activity data'),
          hp(dBioAct, 'Biopsychology, activity data'),
          hp(dSocAct, "Social Psychology, activity data"),
          nrow = 2, ncol = 3,
          font.label = list(size = 9))
```

### Hierarchical cluster number optimisation - method comparison

Choosing to fix the data set on both subjects and activity data only, below compares the different metrics for k-medoids and hierarchical clustering. 

``` {r}
h_plot <- function(data, method = "gap_stat") {
  fviz_nbclust(data, hcut, method = method) +
    ggtitle(label = 'Hierarchical') 
}

k_met_plot <- function(data, method = 'gap_stat') {
  fviz_nbclust(data, cluster::pam, method = method) +
    ggtitle("k-medoids")
}

plist <- list(h_plot(dAct, "gap_stat"),
              h_plot(dAct, "silhouette"),
              k_met_plot(dAct, "gap_stat"),
              k_met_plot(dAct, "silhouette"))
ggarrange(plotlist = plist, nrow = 2, ncol = 2)
```


## Visualising the clusters

When we perform the cluster analysis for various values of k and algorithms the results above also pan out.

### K-Medoids clusters

``` {r}
pl <- map(2:6, function(k)fviz_cluster(cluster::pam(dAct, k))+ggtitle(""))
pl[[length(pl) + 1]] <- fviz_nbclust(dAct, cluster::pam, method = "gap_stat") + ggtitle("")
ggarrange(plotlist = pl)
```

### Hierarchical clustering

This uses the *Ward* method for hierarchical clustering. 

``` {r}
p0 <- fviz_nbclust(dAct, hcut, method = "gap_stat") + ggtitle("")
#p1 <- ggdendro::ggdendrogram(hclust(dist(dAct)), method = 'ward.D')
p2 <-  map(2:6, function(k)fviz_cluster(cluster::pam(dAct, k)) + ggtitle(""))
p2[[(length(p2)+1)]] <- p0
ggarrange(plotlist = p2)
```


# Summary statistics of the clusters

It seems worth exploring both the hierarchical clustering and the k-medoids methods. Cluster grouping seems best at 2, but also possible at clusters of 5 if that yields more interesting groupings of the data. All methods and metrics pushed towards the _activity_ data as being the best for distinct clusters and using both subjects data together.

``` {r}
hc2 <- hcut(dAct, 2)
hc5 <- hcut(dAct, 5)
kmed2 <- pam(dAct, 2)
kmed5 <- pam(dAct, 5)

df <- bind_cols(nodes, data.frame(cluster_hc2 = hc2$cluster,
                                  cluster_hc5 = hc5$cluster,
                                  cluster_kmed2 = kmed2$clustering,
                                  cluster_kmed5 = kmed5$clustering))

# feed in grouped by df
cluster_summary_table <- function(x) {
  x %>%
    summarise(n = n(), 
              Biopsych = sum(Subject == "Biopsychology"),
              SocPsych = sum(Subject == "Social Psychology"),
              Grade = mean(Grade),
              Accesses = mean(Accesses),
              clicks_pa = mean(mean_clicks_per_access),
              sd_clicks = mean(sd_clicks),
              median_time_pa = as.duration(mean(median_time_per_access)),
              sd_time = as.duration(mean(sd_time)),
              TotalTime = as.duration(mean(TotalTime)),
              ForumViews = mean(ForumViews)) %>% 
    knitr::kable()
}

df %>% group_by(cluster_hc2) %>% cluster_summary_table()
df %>% group_by(cluster_kmed2) %>% cluster_summary_table()
df %>% group_by(cluster_hc5) %>% cluster_summary_table()
df %>% group_by(cluster_kmed5) %>% cluster_summary_table()
```

# Summary

## Thoughts

It seems best to choose the k-medoids clustering at 2 and 5, with 5 being possibly more useful for further analysis. 

## Visualization of 5-Medoid clustering of Activity Data

Apologies, the colours from one graph to the next do not match. Cluster 5 is purple in the first graph and light blue in the second 3D graph.

``` {r cluster_plot_3d, webgl = TRUE}
x <- princomp(dAct)$scores #need to generate coordinates
clusters <- kmed5$clustering
fviz_cluster(kmed5)
plot3d(x, col = clusters)
```

# Resources

[^1]: R cluster assessment examples: https://www.datanovia.com/en/blog/types-of-clustering-methods-overview-and-quick-start-r-code/ 

[^2]: Gap statistic: https://statweb.stanford.edu/~gwalther/gap 

[^3]: Unsupervised machine learning: http://www.sthda.com/english/wiki/print.php?id=234

[^4]: PCA, 3D Visualization, and Clustering in R: https://planspace.org/2013/02/03/pca-3d-visualization-and-clustering-in-r/ 

[^5]: PCA: prcomp vs princomp: http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/